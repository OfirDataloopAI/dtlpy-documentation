{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Model Adapter Class Methods  \n", "  \n", "In this tutorial, we can see the flowcharts for the Model Adapter functiions. There are two main types of functions:  \n", "  \n", "* **Wrapper Functions**: Those are functions that come as part of the ```BaseModelAdapter``` class and perform most of the auxiliary tasks needed for adapting a machine learning model to the Dataloop format.  \n", "* **User Functions**: Each wrapper function will call a user function, i.e. a function implemented by the user in their ```ModelAdapter``` with the specific code related to the model they wish to adapt.  \n", "  \n", "In the following sections we will see explanations for each function of each category.  \n", "  \n", "## Wrapper Functions  \n", "  \n", "Here we will see the flowcharts explaining the logic of the wrapper functions. The blocks in green show operations performed by the wrapper function, while blocks in yellow show a call to a user function, which will require implementation.  \n", "  \n", "### `load_from_model`  \n", "  \n", "This function is used to download model artifacts and load them to model object so it will be ready to be used for training and inference.  \n", "  \n", "```mermaid  \n", "flowchart TD  \n", "    id1(load_from_model)-->  \n", "    id2(\"download artifacts @ \\nlocal_path=~/.dataloop/models/{model.name}\")-->  \n", "    id3(\"load(local_path)\")  \n", "    style id1 fill:#adebad, stroke:black  \n", "    style id2 fill:#adebad, stroke:black  \n", "    style id3 fill:#ffff99, stroke:red  \n", "    click id3 \"./#load\"  \n", "```  \n", "  \n", "The ```load_from_model``` function performs starts by downloading artifacts as seen in the ```download_artifacts``` block. It will define the variable ```local_path``` to the path ```~/.dataloop/models/{model.name}``` where ```{model.name}``` will be filled with the name of the model as defined during its creation. Inside this directory, the ```model.artifacts``` will be downloaded. Those usually include weight files, but can include any other auxiliary files needed by the model that were uploaded by the user. More information at [this page](https://developers-dev.redoc.ly/tutorials/model_management/introduction/chapter/#artifacts-and-codebase).  \n", "  \n", "Once all files are in ```local_path``` the user function ```load``` is invoked. Its explanation can be found [below](#load).  \n", "  \n", "The directory structure will be, considering that ```~/.dataloop``` is the default ```DATALOOP_PATH```:  \n", "  \n", "```shell  \n", "Directory tree at this stage:  \n", "DATALOOP_PATH  \n", "|-- models  \n", "|   |-- model.name  \n", "|      |-- artifacts  \n", "```  \n", "  \n", "### `save_to_model`  \n", "  \n", "This function saves the current model files and uploads them as artifacts.  \n", "  \n", "```mermaid  \n", "flowchart TD  \n", "    id1(\"save_to_model\")-->  \n", "    id2(\"save(local_path)\")-->  \n", "    id3(\"upload artifact @ \\nlocal_path/*\")  \n", "    style id1 fill:#adebad, stroke:black  \n", "    style id2 fill:#ffff99, stroke:red  \n", "    style id3 fill:#adebad, stroke:black  \n", "    click id2 \"./#save\"  \n", "```  \n", "  \n", "  \n", "The ```save_to_model``` function invokes the ```save``` user function explained [below](#save) and then uploads all the files in ```local_path``` as model artifacts, as described [here](https://developers-dev.redoc.ly/tutorials/model_management/introduction/chapter/#artifacts-and-codebase).  \n", "  \n", "### `predict_items`  \n", "  \n", "This function receives a list of items over which the model will need to perform predictions and create annotations.  \n", "  \n", "```mermaid  \n", "flowchart TD  \n", "    id1(predict_items)-->  \n", "    id2(i_batch = next batch start)-->  \n", "    id3(\"batch = items[i: (i+batch_size)]\")-->  \n", "    id4(\"prepare_item_func\")-->  \n", "    id5(\"annotation = predict(batch)\")-->  \n", "    id6(\"upload batch_collections for batch_items\")-->  \n", "    id7(\"Is last batch?\")  \n", "    id8(\"return items and annotations list\")  \n", "    id7 -->|No| id3  \n", "    id7 -->|Yes| id8  \n", "    style id1 fill:#adebad, stroke: black  \n", "    style id2 fill:#adebad, stroke: black  \n", "    style id3 fill:#adebad, stroke: black  \n", "    style id4 fill:#ffff99, stroke: red  \n", "    style id5 fill:#ffff99, stroke: red  \n", "    style id6 fill:#adebad, stroke: black  \n", "    style id7 fill:#adebad, stroke: black  \n", "    style id8 fill:#adebad, stroke: black  \n", "    click id4 \"./#prepare_item_func\"  \n", "    click id5 \"./#predict\"  \n", "```  \n", "  \n", "The ```predict_items``` function will prepare batches of items with ```batch_size``` being defined in the model's configurations. For each batch it will call [```prepare_item_func```](#prepare_item_func) which will preprocess the batch's items before they can be used as input to the model in the [```predict(batch)```](#predict) which will have the predictions stored in ```annotation```. The annotations are then stored in ```batch_collections``` and uploaded as annotations for all the items in the batch.  \n", "  \n", "Once the predictions are performed over all the items in all the batches, the function returns a list of items and another list with their respective annotations.  \n", "  \n", "### `train_model`  \n", "  \n", "When running a training session from the model adapter, we start by calling the `train_model` wrapper function so the model can learn according to the data provided to it in its creation.  \n", "  \n", "```mermaid  \n", "flowchart TD  \n", "    id1(train_model)-->  \n", "    id2(load_from_model)-->  \n", "    id3(prepare_data)-->  \n", "    id4(\"train(data_path, out_path)\")-->  \n", "    id5(save_to_model)-->  \n", "    id6(cleanup)  \n", "    style id1 fill:#adebad, stroke: black  \n", "    style id2 fill:#adebad, stroke: black  \n", "    style id3 fill:#adebad, stroke: black  \n", "    style id4 fill:#ffff99, stroke: red  \n", "    style id5 fill:#adebad, stroke: black  \n", "    style id6 fill:#adebad, stroke: black  \n", "    click id2 \"./#load_from_model\"  \n", "    click id4 \"./#train\"  \n", "    click id5 \"./$save_to_model\"  \n", "```  \n", "  \n", "Train model starts by [loading the model](#load_from_model), prepares the data by downloading it according to the directory structure shown below. It then invokes the [```train```](#train) function implemented by the user and saves the result of training by calling [```save_to_model```](#save_to_model). Lastly, it does a ```cleanup``` by deleting all the local copies of the dataset files used for training.  \n", "  \n", "When creating the model, train and validation subsets should be defined for the dataset as shown [here](https://developers.dataloop.ai/tutorials/model_management/create_new_model_ui/chapter/#creating-a-model-from-a-public-architecture). The ```prepare_data``` function will create the directory structure shown below and in the ```data_path``` it will download the training data in the directories seen in this schema. The data will be divided according to the model's subset filters, and ```items``` will hold the data items themselves while ```json``` has the annotation jsons associated to each item. Keep that in mind when preprocessing the data in the ```train``` function.  \n", "  \n", "```shell  \n", "Directory tree at convert_from_dtlpy (supposing train and validation subsets):  \n", "-- DATALOOP_PATH  \n", "   |-- models  \n", "   |   |-- model.name  \n", "   |      |-- artifacts  \n", "   |-- model_data  \n", "       |-- model.id_model.name  \n", "           |-- timestamp (root_path)  \n", "               |-- output (out_path)  \n", "               |-- datasets  \n", "                   |-- dataset.id (data_path)  \n", "                       |-- train  \n", "                       |   |-- items  \n", "                       |   |   |-- train_dir (from filter)  \n", "                       |   |-- json  \n", "                       |       |-- train_dir (from filter)  \n", "                       |-- validation  \n", "                           |-- items  \n", "                           |   |-- val_dir (from filter)  \n", "                           |-- json  \n", "                               |-- val_dir (from filter)  \n", "  \n", "```  \n", "### `evaluate_model`  \n", "  \n", "This function generates predictions for a whole test set provided to it and then creates metrics that will be uploaded to Dataloop platform.  \n", "  \n", "```mermaid  \n", "flowchart TD  \n", "    id1(evaluate_model)-->  \n", "    id2(load_from_model)-->  \n", "    id3(predict_dataset)-->  \n", "    id4(\"evaluate\")  \n", "    style id1 fill:#adebad, stroke: black  \n", "    style id2 fill:#adebad, stroke: black  \n", "    style id3 fill:#adebad, stroke: black  \n", "    style id4 fill:#ffff99, stroke: red  \n", "    click id2 \"./#load_from_model\"  \n", "    click id3 \"./#predict_items\"  \n", "    click id4 \"./#evaluate\"  \n", "```  \n", "  \n", "It starts by invoking [```load_from_model```](#load_from_model) so it has the latest model artifacts and then calls ```predict_dataset``` which in turn calls ```predict_items``` for a whole dataset and a filter provided, which should determine a test set for this dataset. Finally, ```evaluate``` will compute metrics by comparing the model's predictions with the ground truth present in the test set.  \n", "  \n", "## User Function  \n", "  \n", "Those are the function that users must implement in order to run the model. For only prediction, must implement `load` and `predict`. For training - `train` and `save` are also required.  \n", "  \n", "### `load`  \n", "After the wrapper function download all the model artifact to the local directory, users must implement this function to load the model (using the local files and the model config) and instantiate the model.  \n", "  \n", "### `save`  \n", "Users need to implement this function to dump the model state to a local directory, e.g. `torch.save(model.state_dict(), PATH)`  \n", "After that, the wrapper function will take care of the rest and will upload the files into the platform, update the model config, and save everything on to the model entity  \n", "  \n", "### `train`  \n", "This function is called the wrapper function loads the model, downloads and prepare the data.  \n", "Now everything is ready locally and this function implements the actual model training.  \n", "When this is done, there's no need to do anything - the wrapper will take care of the saving and uploading.  \n", "  \n", "### `predict`  \n", "This function is called the load model, so now we have the model ready to predict.  \n", "Each item goes through the `prepare_item_func` and a batch is ready to predict.  \n", "After the model prediction, user will need to prepare the annotation is the Dataloop format using the DL annotations.  \n", "  \n", "### `prepare_item_func`  \n", "Prepares each item for prediction. Bt default, images will be downloaded and loaded into a ndarray as a batch (NHWC)  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}