{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Offline model training, logging metrics in Dataloop  \n", "  \n", "Models can be trained offline (i.e. locally, without connecting the model to the platform) with only model metrics being uploaded to the Dataloop platform for versioning and comparisons.  This tutorial will walk you through how to upload metrics from model training via the SDK.  \n", "  \n", "The Dataloop entities required are:  \n", " - package  \n", " - codebase reference  \n", " - model (with a valid dataset ID)  \n", "  \n", "### Create Dataloop Entities  \n", "First you need to create a dummy package, a dummy codebase reference, and a model with a valid dataset ID. The code below shows how to do this, and remember to replace <project_name> and <dataset_id> with the appropriate strings to reference your project and dataset.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "import os\n", "project = dl.projects.get(project_name='<project_id>')\n", "package = project.packages.push(package_name='dummy-model-package',\n", "                                codebase=dl.entities.LocalCodebase(os.getcwd()),\n", "                                modules=[])\n", "model = package.models.create(model_name='My Model',\n", "                              description='model for offline model logging',\n", "                              dataset_id='<dataset_id>',\n", "                              labels=[])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that you\u2019ve created the necessary Dataloop entities, metrics can be uploaded to the platform with `model.add_log_samples` function.  \n", "  \n", "Here is an example uploading some dummy training data:  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["epoch = np.linspace(0, 9, 10)\n", "epoch_metric = np.linspace(0, 9, 10)\n", "", "for x_metric, y_metric in zip(epoch, epoch_metric):\n", "    model.metrics.create(samples=dl.PlotSample(figure='tutorial plot',\n", "                                               legend='some metric',\n", "                                               x=x_metric,\n", "                                               y=y_metric),\n", "                         dataset_id=model.dataset_id)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Metrics plots will appear under the \u201cmetrics\u201d tab of your chosen model. The above code example will look like this:  \n", "  \n", "![Screenshot of model metrics plot](../../../assets/images/model_management/tutorial_model_metrics.png)  \n", "  \n", "Once you\u2019ve uploaded multiple model metrics, you can compare them by checking all the relevant boxes on the left that you would like to compare.  \n", "  \n", "### List Metrics  \n", "You can list the metrics just like any other entity in the platform - using `list` (and optional dl.Filters):  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["samples = model.metrics.list()\n", "", "for sample in samples.all():\n", "    print(sample.x, sample.y)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}